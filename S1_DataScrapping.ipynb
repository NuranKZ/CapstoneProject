{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#API-urls\" data-toc-modified-id=\"API-urls-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>API-urls</a></span></li><li><span><a href=\"#Company-profiles\" data-toc-modified-id=\"Company-profiles-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Company profiles</a></span></li><li><span><a href=\"#Quotes\" data-toc-modified-id=\"Quotes-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Quotes</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**General Description**\n",
    "\n",
    "- Code for Data Scrapping to local project space is presented in this notebook\n",
    "- Data Source is specialized web-site https://financialmodelingprep.com/, which provides greatest and simple API for loading financial data and which not requires Registration and Autentification\n",
    "- The data loaded in json and csv formats\n",
    "- Due to the fact, that data loading takes several hours, some parts of the code for that were shown in illustrative purposes and commented for safety reason \n",
    "- loaded data have about 0.3 Gb and contains ~10k files\n",
    "- due to GitHub limitations, I've put loaded files on my Google Drive with this [link](https://drive.google.com/open?id=11Zw-DvNbpc_lc3kj4AFzwESaKZ6Rs-0M)\n",
    "  - raw loaded data is located in `data.rar`\n",
    "  - if the project forked, this file need to be unzipped to project folder\n",
    "\n",
    "\n",
    "**Data Loading Process**\n",
    "\n",
    "- I've loaded here company profiles and quotes (prices)\n",
    "- Financial Statements (on quarter basis) will be loaded only on selected (after clusterization) companies in Step-7 of the Project\n",
    "- Manual Functions and Classes used here and in other Steps of the Project are imported from `project_lib.py`\n",
    "\n",
    "- Company Profiles: some part of Data Cleaning and preprocessing for Company profiles have been done here:\n",
    "  - dropping dublicates\n",
    "  - company filtering\n",
    "  - saving in DataFrame (csv)\n",
    "  \n",
    "- Company quotes: loaded only for filtered company list (from Company Profiles) as separate CSV for each company\n",
    "- For safety purposes some code parts here - It is recommended to not run this code, all data already loaded and saved to GoogleDrive\n",
    "\n",
    "\n",
    "**Companies filtering**  \n",
    "\n",
    "The Project main target - is to create robust predictors for some public companies. That's why I decided to exclude from loaded tickers some composite-type indeces which could be formulated as some linear combinations of initial public companies. The following groups were excluded in filtering stage\n",
    "- General Composite Indeces such as Dow Jones and S&P (full list - in `index_names` 2.2)\n",
    "- Any other tickers with keyword `index`\n",
    "- ETF and Mutual Funds\n",
    "- Cryptocurrencies\n",
    "\n",
    "After the filtering total tickers numbers reduced from 12 138 to 10 450\n",
    "\n",
    "\n",
    "**Functions used in this notebook**\n",
    "- `get_jsonparsed_data` - makes get-request for json-type data\n",
    "- `find_shablons` - used for finding keyword in list of ticker names (for filtering purposes)\n",
    "- `dt_to_string` - transoform datetime format into given STR-format\n",
    "  - this function is used in another function: `url_for_stock`\n",
    "- `url_for_stock` - used for generate API-URL for company daily quotes with given start and end dates\n",
    "\n",
    "**Local Space Structure for loaded Data**\n",
    "- base folder for loaded data: `data`\n",
    "- company profiles (`profiles.csv`) - in base folder\n",
    "- company daily quotes ({`company_symbol.csv`}) - in `data/quotes`\n",
    "- company quartely financial statements ({`company_symbol_fsq.csv`}) - in `data/fsq`\n",
    "  - this files will be loaded later, in Step-7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### API-urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.1. necessary modules\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "import csv\n",
    "import requests\n",
    "import time\n",
    "import warnings\n",
    "from datetime import date as dt\n",
    "import pickle\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.2. main API URLS\n",
    "COMP_LIST = \"https://financialmodelingprep.com/api/v3/company/stock/list\"\n",
    "PROFILE_per_COMP = f\"https://financialmodelingprep.com/api/v3/company/profile/\"\n",
    "ETF_LIST = \"https://financialmodelingprep.com/api/v3/symbol/available-etfs\"\n",
    "MF_LIST = \"https://financialmodelingprep.com/api/v3/symbol/available-mutual-funds\"\n",
    "CRYPTO_LIST = \"https://financialmodelingprep.com/api/v3/cryptocurrencies\"\n",
    "QUOTES_BASE_URL = \"https://financialmodelingprep.com/api/v3/historical-price-full/\"\n",
    "COMMODITIES_LIST_URL = \"https://financialmodelingprep.com/api/v3/symbol/available-commodities\"\n",
    "COMMODITIES_BASE_URL = \"https://financialmodelingprep.com/api/v3/historical-price-full/commodity/\"\n",
    "URL_IS_Q_JSON = \"https://financialmodelingprep.com/api/v3/financials/income-statement/\"\n",
    "URL_BS_Q_JSON = \"https://financialmodelingprep.com/api/v3/financials/balance-sheet-statement/\"\n",
    "URL_CF_Q_JSON = \"https://financialmodelingprep.com/api/v3/financials/cash-flow-statement/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.3. necessary functions from project_lib module\n",
    "from project_lib import get_jsonparsed_data, find_shablons, dt_to_string, url_for_stock"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Company profiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.1. Parsing tickers list and profiles with cleaning\n",
    "\n",
    "tickers = get_jsonparsed_data(COMP_LIST)['symbolsList']\n",
    "df_all_tickers = pd.DataFrame(tickers)\n",
    "df_all_tickers = df_all_tickers.drop(columns=['price'], axis=1)\n",
    "name_list = df_all_tickers.name.tolist()\n",
    "\n",
    "#print(f'total {len(name_list)} tickers parsed, including {len(name_list) - len(set(name_list))} dublicates')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.2. Data Cleaning (taking off composite indeces and ETF)\n",
    "index_names = ['Dow Jones', 'S&P', 'AMEX', 'NASDAQ', 'NYSE', 'Russel','Wilshire', 'CAC', 'DAX', 'FTSE', \n",
    "               'Nikkei', 'TSE', 'index', 'etf']\n",
    "\n",
    "overall_set = set()\n",
    "for i in index_names:\n",
    "    temp_list = set(find_shablons(i, name_list))\n",
    "    overall_set.update(temp_list)\n",
    "overall_set = list(overall_set)\n",
    "\n",
    "tickers_to_drop = []\n",
    "for name, ticker in zip(name_list, df_all_tickers.symbol):\n",
    "    if name in overall_set:\n",
    "        tickers_to_drop.append(ticker)\n",
    "\n",
    "comp_tickers_list = list(set(df_all_tickers.symbol) - set(tickers_to_drop))\n",
    "#print(f'{len(comp_tickers_list)} company tickers in total')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.3. Additional Data Cleaning (taking off ETF, Mutual Funds and CryptoCurrencies)\n",
    "tickers_to_drop = []\n",
    "    \n",
    "for url in [ETF_LIST, MF_LIST, CRYPTO_LIST]: \n",
    "    data = get_jsonparsed_data(url)\n",
    "    if url == CRYPTO_LIST:\n",
    "        df = pd.DataFrame(data['cryptocurrenciesList'])\n",
    "        temp_list = list(df['ticker'].values)\n",
    "    else:\n",
    "        df = pd.DataFrame(data)\n",
    "        temp_list = list(df['symbol'].values)\n",
    "    tickers_to_drop.extend(temp_list)\n",
    "\n",
    "comp_tickers_list = list(set(comp_tickers_list) - set(tickers_to_drop))\n",
    "#print(f'{len(comp_tickers_list)} company tickers in total excluding funds and crypto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.4 Loading company profiles\n",
    "# -- Atention: it can take several hours\n",
    "\n",
    "temp_list = []\n",
    "progress_scanner_int = 500\n",
    "idx = 0\n",
    "total_items = len(comp_tickers_list)\n",
    "\n",
    "start_time = time.time()\n",
    "local_start_time = time.time()\n",
    "\n",
    "for ticker in comp_tickers_list:\n",
    "    idx += 1\n",
    "    url = PROFILE_per_COMP+ticker\n",
    "    data = get_jsonparsed_data(url)['profile']\n",
    "    data['symbol'] = ticker\n",
    "    temp_list.append(data)\n",
    "    if idx%progress_scanner_int == 0:\n",
    "        print(f'{idx} items ({round(100*idx/total_items, 1)}%) loaded ...')\n",
    "        print(f'time: {time.time()-local_start_time} seconds')\n",
    "        print(f'{(time.time()-local_start_time)/progress_scanner_int} seconds per 1 company')\n",
    "        print(50*'.')\n",
    "        local_start_time = time.time()\n",
    "end_time = time.time()\n",
    "total_time = end_time - start_time\n",
    "\n",
    "df_reduced_profiles = pd.DataFrame(temp_list)\n",
    "df_columns = ['symbol', 'companyName', 'sector', 'industry', 'exchange', 'description', 'ceo', 'mktCap']\n",
    "df_reduced_profiles = df_reduced_profiles[df_columns]\n",
    "\n",
    "#print(f'profiles for {len(df_reduced_profiles)} companies loaded in {total_time} sec')\n",
    "#display(df_reduced_profiles.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.5 Save dataset to CSV\n",
    "with open('data/profiles.csv', 'w', encoding='utf-8') as f:\n",
    "    df_reduced_profiles.to_csv(f, columns=df_reduced_profiles.columns) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quotes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.1. Quotes loading\n",
    "# ATTENTION: it will take up to 3 hours\n",
    "\n",
    "progress_scanner_int = 500\n",
    "idx = 0\n",
    "total_items = len(comp_tickers_list)\n",
    "\n",
    "start_time = time.time()\n",
    "local_start_time = time.time()\n",
    "date_for_quotes = dt(2016, 1, 1)\n",
    "\n",
    "\n",
    "for ticker in comp_tickers_list:\n",
    "    idx += 1\n",
    "    url = url_for_stock(QUOTES_BASE_URL, ticker, start_date_in_dt=date_for_quotes)\n",
    "    data = get_jsonparsed_data(url)\n",
    "    try:\n",
    "        df = pd.DataFrame(data['historical'])[['date', 'close', 'volume']]\n",
    "        filename = ticker+'_quotes.csv'\n",
    "        path = f'data/quotes/{filename}'\n",
    "        try:\n",
    "            with open(path, 'w', encoding='utf-8') as f:\n",
    "                df.to_csv(f, encoding='utf-8', sep=',', columns=df.columns) \n",
    "        except Exception:\n",
    "            pass\n",
    "    except Exception:\n",
    "        pass\n",
    "    \n",
    "    if idx%progress_scanner_int == 0:\n",
    "        print(f'{idx} tables ({round(100*idx/total_items, 1)}%) loaded ...')\n",
    "        print(f'time: {time.time()-local_start_time} seconds')\n",
    "        print(f'{(time.time()-local_start_time)/progress_scanner_int} seconds per 1 company')\n",
    "        print(50*'.')\n",
    "        local_start_time = time.time()\n",
    "\n",
    "end_time = time.time()\n",
    "total_time = end_time - start_time\n",
    "\n",
    "#print(f'historical quotes for {idx} companies loaded in {total_time} sec')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_Gpu",
   "language": "python",
   "name": "tensorflow_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "290.512px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
